{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IMPORTS ALL THE NECESSARY LIBRARIES TO RUN THE CODE\n",
    "\n",
    "# Is_To_Run_On_Cloud = True\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import json\n",
    "# Is_HPC = True\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "# if(Is_HPC):\n",
    "#     pass\n",
    "# else:\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers.core import Flatten, Dense,Dropout\n",
    "from keras import Model\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "from time import sleep\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.python.client import device_lib\n",
    "# if(Is_To_Run_On_Cloud):\n",
    "#     from keras import keras_applications as K_Models\n",
    "# else:\n",
    "from keras import applications as K_Models\n",
    "# Is_Running_On_Local_Desktop = True\n",
    "# FAA_HPC = False\n",
    "from classification_models.resnet import ResNet18, ResNet34, preprocess_input\n",
    "\n",
    "    \n",
    "# if(Training_Gauges[\"True Airspeed(kn)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL CREATES LENNET MODEL ARCHITECTURE\n",
    "\n",
    "PADDING = \"SAME\" # \"VALID\"\n",
    "def LenNet(input_size, output_classes):\n",
    "    # Vanilla CNN. Play around with architecture.\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 64, \n",
    "                     kernel_size = 9, \n",
    "                     strides = 1, \n",
    "                     activation = 'relu', \n",
    "                     padding=PADDING,\n",
    "                     input_shape = (input_size[0],input_size[1],input_size[2])))\n",
    "    model.add(Conv2D(filters = 64, \n",
    "                     kernel_size = 7, \n",
    "                     strides = 1, \n",
    "                     activation = 'relu',\n",
    "                     padding=PADDING,\n",
    "                    ))\n",
    "    model.add(Conv2D(filters = 64, \n",
    "                     kernel_size = 5, \n",
    "                     strides = 1, \n",
    "                     activation = 'relu', \n",
    "                     padding=PADDING,\n",
    "                     ))\n",
    "    model.add(MaxPooling2D(pool_size = 5, strides = 1))\n",
    "    model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 padding=PADDING,\n",
    "                ))\n",
    "    model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 padding=PADDING,\n",
    "                 activation = 'relu'\n",
    "                 ))\n",
    "    model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = 3,\n",
    "                 strides = 1,\n",
    "                 padding=PADDING,\n",
    "                 activation = 'relu'\n",
    "                 ))\n",
    "    model.add(MaxPooling2D(pool_size = 5, strides = 1))\n",
    "    model.add(Conv2D(filters = 64, \n",
    "                     kernel_size = 3,\n",
    "                     strides = 1,\n",
    "                     activation = 'relu',\n",
    "                     padding=PADDING\n",
    "                     ))\n",
    "    model.add(Conv2D(filters = 64, \n",
    "                     kernel_size = 3,\n",
    "                     strides = 1,\n",
    "                     activation = 'relu',\n",
    "                     padding=PADDING\n",
    "                    ))\n",
    "    model.add(MaxPooling2D(pool_size = 3, strides = 2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_classes, activation='softmax', name='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### DEFINE THE FUNCTIONS USED IN THE FINAL FUNCTION TO DO TRAINING ######\n",
    "\n",
    "##############################################################################################\n",
    "# 1. Load the correct model.\n",
    "# INPUT -> SELECTED_MODEL (must be from list of available models)\n",
    "# OUTPUT -> ML_Model\n",
    "def Load_Model():\n",
    "    global ML_Model\n",
    "    if(SELECTED_MODEL == Available_Models.VGG16):\n",
    "        ML_Model = K_Models.VGG16(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.VGG19):\n",
    "        ML_Model = K_Models.VGG19(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.InceptionV3):\n",
    "        ML_Model = K_Models.InceptionV3(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.ResNet50):\n",
    "        ML_Model = K_Models.ResNet50(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.ResNet18):\n",
    "        ML_Model = ResNet18(input_shape=INPUT_SIZE, weights=INITIALIZATION_WIEGHTS, include_top=INCLUDE_TOP)\n",
    "\n",
    "    elif(SELECTED_MODEL == Available_Models.ResNet34):\n",
    "        ML_Model = ResNet34(input_shape=INPUT_SIZE, weights=INITIALIZATION_WIEGHTS, include_top=INCLUDE_TOP)\n",
    "        \n",
    "    elif(SELECTED_MODEL == Available_Models.DenseNet121):\n",
    "        ML_Model = K_Models.DenseNet121(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.DenseNet169):\n",
    "        ML_Model = K_Models.DenseNet169(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.DenseNet201):\n",
    "        ML_Model = K_Models.DenseNet201(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.MobileNet):\n",
    "        ML_Model = K_Models.MobileNet(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "\n",
    "    elif(SELECTED_MODEL == Available_Models.InceptionResNetV2):\n",
    "        ML_Model = K_Models.InceptionResNetV2(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.NASNetLarge):\n",
    "        ML_Model = K_Models.NASNetLarge(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                            pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.Xception):\n",
    "        ML_Model = K_Models.Xception(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.NASNetMobile):\n",
    "        ML_Model = K_Models.NASNetMobile(include_top=INCLUDE_TOP, weights=INITIALIZATION_WIEGHTS, \n",
    "                                                              input_tensor=INPUT_TENSOR, input_shape=INPUT_SIZE,\n",
    "                                                              pooling=POOLING, classes=OUTPUT_CLASSES)\n",
    "    elif(SELECTED_MODEL == Available_Models.Lenet_Style):\n",
    "        ML_Model = LenNet(input_size= INPUT_SIZE, output_classes=OUTPUT_CLASSES)\n",
    "        return\n",
    "    else:\n",
    "        print(\"NO MODEL IDENTIFIER MATCHED\")\n",
    "        \n",
    "    # Add the output layer after the model architecture\n",
    "    if(POOLING is None):\n",
    "        last_layer = Flatten()(ML_Model.output)\n",
    "    else:\n",
    "        last_layer = ML_Model.output\n",
    "        if(SELECTED_MODEL ==  Available_Models.ResNet18 or SELECTED_MODEL == Available_Models.ResNet34):\n",
    "            last_layer = keras.layers.GlobalAveragePooling2D()(ML_Model.output)\n",
    "        \n",
    "    if(len(Dense_Layers_Neurons) > 0):\n",
    "        for n in Dense_Layers_Neurons:\n",
    "            last_layer = Dense(n, activation='relu')(last_layer)\n",
    "            if(Is_To_Add_Dropout):\n",
    "                last_layer =  Dropout(Dropout_Rate)\n",
    "    else:\n",
    "        if(Is_To_Add_Dropout):\n",
    "    #         for neurons in Dense_Layers:\n",
    "    #             last_layer = Dense(neurons, activation='relu')(last_layer)\n",
    "            last_layer =  Dropout(Dropout_Rate)(last_layer)\n",
    "        \n",
    "    last_layer = Dense(OUTPUT_CLASSES, activation=FINAL_LAYER_CLASSIFICATION_FUCNTION,\n",
    "                       name='softmax')(last_layer)\n",
    "    \n",
    "    # This is the output of this function, feeds into function 2\n",
    "    ML_Model = Model(ML_Model.input, last_layer)\n",
    "    \n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "# 2. Regularize Layers.\n",
    "# INPUT -> ML_Model, REGULARIZE_EARLY_LAYERS_WIHT_PERCENT, REGULARIZE_EARLY_LAYERS_WITH_RATIO\n",
    "# OUTPUT -> ML_Model with regularized layers\n",
    "def Add_Conv_Regularizer():\n",
    "    global ML_Model\n",
    "#     alpha = 0.0001  # weight decay coefficient\n",
    "    if(REGULARIZE_EARLY_LAYERS_WIHT_PERCENT <= 0):\n",
    "        # Do Not Regularize Any Layers\n",
    "        print(\"Zero layers regularized\")\n",
    "    else:\n",
    "        # Regularizing Models Layers\n",
    "        regularized_layers_count = int(len(ML_Model.layers) * REGULARIZE_EARLY_LAYERS_WIHT_PERCENT)\n",
    "        print(\"Regularize {0} early layers\".format(regularized_layers_count))\n",
    "        for layer in ML_Model.layers[:regularized_layers_count]:\n",
    "            \n",
    "            if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense):\n",
    "#                 print(\"Before Regularization Added=\" + str(layer.losses))\n",
    "                layer.add_loss(keras.regularizers.l2(REGULARIZE_EARLY_LAYERS_WITH_RATIO)(layer.kernel))\n",
    "#                 print(\"Layer Name=\", layer.name)\n",
    "#                 print(\"After Regularization Added=\" + str(layer.losses))\n",
    "            if hasattr(layer, 'bias_regularizer') and layer.use_bias:\n",
    "                layer.add_loss(keras.regularizers.l2(REGULARIZE_EARLY_LAYERS_WITH_RATIO)(layer.bias))\n",
    "            \n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "# 3. Determines if you want to freeze layers or not.\n",
    "# INPUT -> ML_Model, FREEZE_EARLY_LAYERS_WIHT_PERCENT\n",
    "# OUTPUT -> ML_Model with frozen layers\n",
    "def Freeze_Model_Parameters():#ML_Model, FREEZE_EARLY_LAYERS_WIHT_PERCENT):\n",
    "    global ML_Model\n",
    "    if(FREEZE_EARLY_LAYERS_WIHT_PERCENT <= 0):\n",
    "        # Do Not Freeze any Layers\n",
    "        print(\"Zero layers freezed\")\n",
    "    else:\n",
    "        # Freezing Models Layers\n",
    "        freezed_layers_count = int(len(ML_Model.layers) * FREEZE_EARLY_LAYERS_WIHT_PERCENT)\n",
    "        print(\"Freeze {0} early layers\".format(freezed_layers_count))\n",
    "        for layer in ML_Model.layers[:freezed_layers_count]:\n",
    "            layer.trainable = False    \n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "# 4. Compile the Model.\n",
    "# INPUT -> ML_Model, LEARNING_RATE\n",
    "# OUTPUT -> P_ML_Model (compiled model)\n",
    "def Compile_Model():\n",
    "    global P_ML_Model\n",
    "    global ML_Model\n",
    "\n",
    "    if(AVAILABLE_GPUs > 1):\n",
    "        print(\"Compiling multi GPU models\")\n",
    "        P_ML_Model = multi_gpu_model(ML_Model, AVAILABLE_GPUs)\n",
    "    else:\n",
    "        print(\"Compiling single GPU model\")\n",
    "        P_ML_Model = ML_Model\n",
    "    \n",
    "    # set the adam optimizer\n",
    "    adam = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999,\n",
    "            epsilon=None, decay=1e-6, amsgrad=False)\n",
    "    \n",
    "    ##### THIS IS THE OUTPUT #####\n",
    "    P_ML_Model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "# 5. This loads in the data from the correct folder. \n",
    "# INPUT -> Is_To_Apply_Data_Augmentation, INPUT_SIZE, BATCH_SIZE\n",
    "# OUTPUT -> TRAIN_GENERATOR, VALID_GENERATOR, STEP_SIZE_TRAIN, STEP_SIZE_VALID. These get fed into function 7\n",
    "def Create_Data_Loader():\n",
    "    global TRAIN_GENERATOR\n",
    "    global VALID_GENERATOR\n",
    "    global STEP_SIZE_TRAIN\n",
    "    global STEP_SIZE_VALID\n",
    "    \n",
    "    if(Is_To_Apply_Data_Augmentation):\n",
    "        # Apply Data Augmentation to the input images\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "#             rotation_range=40,\n",
    "#             width_shift_range=0.2,\n",
    "#             height_shift_range=0.2,\n",
    "#             shear_range=0.2,\n",
    "#             zoom_range=[-0.2, 0.2],\n",
    "#             horizontal_flip=True,\n",
    "#             fill_mode='nearest'\n",
    "        )\n",
    "        # normalize the validation dataset\n",
    "        validation_datagen = ImageDataGenerator(\n",
    "            rescale=1./255)\n",
    "    else:\n",
    "        # normalize the input data for training and validation\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Create the TRAINING GENERATOR\n",
    "    TRAIN_GENERATOR = train_datagen.flow_from_directory(TRAINING_DIRECTORY_PATH,  # this is the target directory\n",
    "                classes =CLASS_MAPPINGS,\n",
    "                target_size=(INPUT_SIZE[0], INPUT_SIZE[1]),  # all images will be resized \n",
    "                batch_size=BATCH_SIZE,\n",
    "                color_mode=COLOR_MODE,\n",
    "                shuffle=SHUFFLE,\n",
    "                seed=SEED,\n",
    "                class_mode='categorical') \n",
    "    print(\"TRAINING_DIRECTORY_PATH=\",TRAINING_DIRECTORY_PATH)\n",
    "    print(\"TRAIN_GENERATOR.class_indices =\", TRAIN_GENERATOR.class_indices)\n",
    "    \n",
    "    # Create the VALIDATION GENERATOR\n",
    "    VALID_GENERATOR = validation_datagen.flow_from_directory(\n",
    "                VALIDATION_DIRECTORY_PATH,\n",
    "                classes =CLASS_MAPPINGS,\n",
    "                target_size=(INPUT_SIZE[0], INPUT_SIZE[1]),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                color_mode=COLOR_MODE,\n",
    "                seed=SEED,\n",
    "                class_mode='categorical')\n",
    "    print(\"VALIDATION_DIRECTORY_PATH=\",VALIDATION_DIRECTORY_PATH)\n",
    "    print(\"TRAIN_GENERATOR.class_indices =\", VALID_GENERATOR.class_indices)\n",
    "\n",
    "    # Calculate STEP SIZE for Training and Validation\n",
    "    STEP_SIZE_TRAIN=TRAIN_GENERATOR.n//TRAIN_GENERATOR.batch_size\n",
    "    STEP_SIZE_VALID=VALID_GENERATOR.n//VALID_GENERATOR.batch_size\n",
    "    print(\"STEP_SIZE_TRAIN=\",STEP_SIZE_TRAIN)\n",
    "    print(\"STEP_SIZE_VALID=\",STEP_SIZE_VALID)\n",
    "##############################################################################################    \n",
    "\n",
    "##############################################################################################\n",
    "# 6. Create the paths to where the data will be saved. \n",
    "# INPUT -> SELECTED_MODEL, C_Guage, Store_Trained_ML_Model_Info_At\n",
    "# OUTPUT -> The paths are set to where the info will be saved\n",
    "def Get_Model_Storage_Info():\n",
    "    global Model_Name\n",
    "    global Weights_Path\n",
    "    global History_Path\n",
    "    global Model_Path\n",
    "\n",
    "    Model_Name = \"{0}_{1}\".format(\n",
    "        SELECTED_MODEL, C_Guage)\n",
    "    \n",
    "    Weights_Path = \"{0}/CheckPoint_{1}.hdf5\".format(\n",
    "                    Store_Trained_ML_Model_Info_At, Model_Name)\n",
    "    \n",
    "    History_Path = \"{0}/{1}_history\".format(Store_Trained_ML_Model_Info_At, \n",
    "                                            Model_Name)\n",
    "\n",
    "    Model_Path = \"{0}/{1}\".format(Store_Trained_ML_Model_Info_At,\n",
    "                                                          Model_Name)\n",
    "    \n",
    "    # PRINT THE PATHS\n",
    "    print(\"Model_Name=\", Model_Name)\n",
    "    print(\"Weights_Path=\", Weights_Path)\n",
    "    print(\"History_Path=\", History_Path)\n",
    "    print(\"Model_Path=\", Model_Path)\n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "# 7. Do the actual training of the model\n",
    "# INPUT -> Outputs from FUNCTION 5, \n",
    "# OUTPUT -> History file\n",
    "def Do_Training():\n",
    "\n",
    "    global History\n",
    "    # Schedule a Learning Rate\n",
    "    #lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    # Keep track of the model at each epoch and only save the model if it improves from last epoch\n",
    "    checkpoint = ModelCheckpoint(Weights_Path, monitor=Monitor, verbose=1, save_best_only=True, mode=Mode)\n",
    "    # set the early stopping criteria, EARLY STOPPING PATIENCE\n",
    "    early_stopping = EarlyStopping(monitor=Monitor, min_delta=0, patience=EARLY_STOPPING_PATIENCE, verbose=0, \n",
    "                                       mode=Mode)\n",
    "\n",
    "    #     lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "    print(\"CLASSES_WEIGTHAGES=\", CLASSES_WEIGTHAGES)\n",
    "    callbacks_list = [early_stopping,  checkpoint]#, lr_scheduler]\n",
    "    \n",
    "    # DO THE TRAINING\n",
    "    History = P_ML_Model.fit_generator(\n",
    "            TRAIN_GENERATOR,\n",
    "            steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "            epochs=EPOCHs,\n",
    "            validation_data=VALID_GENERATOR,\n",
    "            validation_steps=STEP_SIZE_VALID, \n",
    "             callbacks=callbacks_list,\n",
    "            class_weight = CLASSES_WEIGTHAGES)\n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "# 8. Saves the info as a .json file?\n",
    "# INPUT -> History_Path\n",
    "# OUTPUT -> Model is Saved\n",
    "def Save_Model_Info():\n",
    "    with open('{0}.json'.format(History_Path), 'w') as f:\n",
    "        json.dump(History.history, f)\n",
    "\n",
    "    pd.DataFrame(History.history).to_csv(\"{0}.csv\".format(History_Path))\n",
    "    try:\n",
    "        model_yaml = ML_Model.to_yaml()\n",
    "        with open(\"{0}.yaml\".format(Model_Path), \"w\") as yaml_file:\n",
    "            yaml_file.write(model_yaml)\n",
    "    except Exception as eExcep:\n",
    "        print(str(eExcep))\n",
    "    model_json = ML_Model.to_json()\n",
    "    with open(\"{0}.json\".format(Model_Path), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "#     ML_Model.save(Model_Path)\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "# 9. This is for evaluating the test set. I'll be doing this with my own function instead. \n",
    "def Evaluate_Model():\n",
    "    \n",
    "    global TEST_CSV\n",
    "    try:\n",
    "        TEST_CSV[\"{0}_Prd_{1}_Selected_Model_{2}\".format(C_Guage,Model_Name, SELECTED_MODEL)] = None\n",
    "        try:\n",
    "            for index, row in TEST_CSV.iterrows():\n",
    "#                 try:\n",
    "                if(index != 0 and index % 2500 == 0):\n",
    "                    print(\"Predicted Examples Count={0}\".format(index))\n",
    "#                     break\n",
    "                for test_img_path in TEST_CSV[\"ML_File_Path_For_TrainValTest\"][index].split(\",\"):\n",
    "#                     print(\"test_img_path=\", test_img_path)\n",
    "                    try:\n",
    "                        test_img_path = test_img_path.replace(\"./\", \"{0}/\".format(Dell_Path))\n",
    "    #                     print(\"test_img_path=\", test_img_path)\n",
    "                        test_img = cv2.imread(test_img_path, cv2.IMREAD_COLOR)\n",
    "                        test_img = test_img.astype(\"float32\") / 255\n",
    "                        #                     test_img = np.expand_dims(test_img, -1)\n",
    "                        test_img = cv2.resize(test_img, (INPUT_SIZE[0], \n",
    "                                                           INPUT_SIZE[1]))\n",
    "                        test_img = np.reshape(test_img,(1,test_img.shape[0], \n",
    "                                    test_img.shape[1], test_img.shape[2]))\n",
    "                        prediction = np.argmax(ML_Model.predict(test_img, \n",
    "                                                            batch_size=1))\n",
    "                        TEST_CSV[\"{0}_Prd_{1}_Selected_Model_{2}\".format(C_Guage,Model_Name, SELECTED_MODEL)][index] = prediction\n",
    "                        #                 if(TEST_CSV[\"{0}_Prd_{1}\".format(C_Guage,Model_Name)] = None)\n",
    "                        #                 TEST_CSV[\"{0}_Prd_{1}\".format(C_Guage,Model_Name)][index] = prediction\n",
    "    #                     print(\"Prediction=\" + str(prediction))\n",
    "                    except Exception as eERRoR:\n",
    "                        TEST_CSV[\"{0}_Prd_{1}_Selected_Model_{2}\".format(C_Guage,Model_Name, SELECTED_MODEL)][index] = -1\n",
    "                    break\n",
    "#                 except Exception as eError:\n",
    "#                     TEST_CSV[\"{0}_Prd_{1}\".format(C_Guage, Model_Name)][index] = \"Error\"\n",
    "#                     print(\"Some Error Occurred While Prediction.\" + str(eError))\n",
    "#                     continue\n",
    "        except Exception as eError:\n",
    "            TEST_CSV[\"{0}_Prd_{1}_Selected_Model_{2}\".format(C_Guage,Model_Name, SELECTED_MODEL)][index] = \"Error\"\n",
    "            print(\"Some Error Occurred While Prediction.\" + str(eError))\n",
    "\n",
    "    except Exception as eError:\n",
    "        print(\"Some Error Occurred While Evaluating the Model.\" + str(eError))\n",
    "    \n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# This is the main model training function.\n",
    "def Initiate_Model_Training():\n",
    "    global ML_Model\n",
    "    global P_ML_Model\n",
    "    \n",
    "    print(\"SELECTED MODEL #\" + str(SELECTED_MODEL))\n",
    "    if(AVAILABLE_GPUs > 1):\n",
    "        with tf.device('/cpu:0'):\n",
    "            Load_Model()\n",
    "    else:\n",
    "        Load_Model()\n",
    "    Add_Conv_Regularizer()\n",
    "    Freeze_Model_Parameters()\n",
    "    Compile_Model()\n",
    "    Create_Data_Loader()\n",
    "    Get_Model_Storage_Info()\n",
    "    ML_Model.summary()\n",
    "    Do_Training()\n",
    "    Save_Model_Info()\n",
    "#   Evaluate_Model()\n",
    "\n",
    "    #Release Resources\n",
    "    ML_Model = None\n",
    "    P_ML_Model = None\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    sleep(COOL_DOWN)\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE_GPUs= 1\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "# MODEL SELECTION HELP OR AVAILABLE MODELS\n",
    "class Available_Models(Enum):\n",
    "    VGG16 = \"VGG16\"\n",
    "    VGG19 = \"VGG19\"\n",
    "    InceptionV3 = \"InceptionV3\"\n",
    "    Xception = \"Xception\" # TensorFlow ONLY\n",
    "    ResNet50 = \"ResNet50\"\n",
    "    ResNet18 = \"ResNet18\"\n",
    "    ResNet34 = \"ResNet34\"\n",
    "    DenseNet121 = \"DenseNet121\"\n",
    "    DenseNet169 = \"DenseNet169\"\n",
    "    DenseNet201 = \"DenseNet201\"\n",
    "    MobileNet = \"MobileNet\"\n",
    "    InceptionResNetV2 = \"InceptionResNetV2\"\n",
    "    NASNetLarge = \"NASNetLarge\"\n",
    "    NASNetMobile = \"NASNetMobile\"\n",
    "    Lenet_Style = \"Lenet_Style\"\n",
    "##############################################################################################\n",
    "\n",
    "# Global Configurations\n",
    "# Hyper-parameters Configuration etc\n",
    "\n",
    "# Initialize all the the global parameters\n",
    "C_Guage = None\n",
    "ML_Model = None\n",
    "P_ML_Model = None\n",
    "TRAIN_GENERATOR = None\n",
    "VALID_GENERATOR = None\n",
    "STEP_SIZE_TRAIN = None\n",
    "STEP_SIZE_VALID = None\n",
    "Model_Name = None\n",
    "Weights_Path = None\n",
    "History_Path = None\n",
    "Model_Path = None\n",
    "History = None\n",
    "\n",
    "# Initialize paths, these are all set in the next cell\n",
    "# BASE = \".\"                               # Dont think this is needed\n",
    "ROOT_DS_FOLDER_PATH = None                 # Dell_Path, root folder containing TRAINING, VALIDATION, TEST DIRECTORY\n",
    "TRAINING_DIRECTORY_PATH = None             # Dell_path/TRAINING_DIRECTORY\n",
    "VALIDATION_DIRECTORY_PATH = None           # Dell_path/VALIDATION_DIRECTORY\n",
    "TESTING_DIRECTORY_PATH = None              # Dell_path/TEST_DIRECTORY\n",
    "Store_Trained_ML_Model_Info_At = None      # ./Trained_Models/C_Guage\n",
    "\n",
    "##### FUNCTION 1 INPUTS #####\n",
    "OUTPUT_CLASSES = 9                                # Tell the model how many classes there are, This is set in MAIN LOOP\n",
    "FINAL_LAYER_CLASSIFICATION_FUCNTION = \"softmax\"   # Sets the activation function for the output layer\n",
    "POOLING = None                                    # avg pooling, max pooling, or None for flatten\n",
    "INITIALIZATION_WIEGHTS = \"imagenet\"               # intialize the weights as the imagenet weights\n",
    "INCLUDE_TOP = False\n",
    "INPUT_TENSOR = None\n",
    "\n",
    "##### FUNCTION 2 INPUTS #####\n",
    "\n",
    "##### FUNCTION 3 INPUTS #####\n",
    "FREEZE_EARLY_LAYERS_WIHT_PERCENT = 0\n",
    "\n",
    "##### FUNCTION 4 INPUTS, Compile_Model() #####\n",
    "# Make sure to use the GPUs if available when compiling the model\n",
    "AVAILABLE_GPUs = len([gpu for gpu in device_lib.list_local_devices() if gpu.device_type==\"GPU\"])\n",
    "if(AVAILABLE_GPUs == 0):\n",
    "    AVAILABLE_GPUs = 1\n",
    "    print(\"AVAILABLE_GPUs = 0 (NO GPU ACCESS)\")\n",
    "print(\"AVAILABLE_GPUs=\",AVAILABLE_GPUs)\n",
    "\n",
    "##### FUNCTION 5 INPUTS #####\n",
    "SEED = 101\n",
    "COLOR_MODE = \"rgb\"\n",
    "SHUFFLE = True\n",
    "CLASS_MAPPINGS = []              # set in MAIN LOOP\n",
    "BATCH_SIZE = 0                   # Must be zero here \n",
    "\n",
    "##### FUNCTION 6 INPUTS #####\n",
    "\n",
    "##### FUNCTION 7 INPUTS #####\n",
    "Monitor = \"val_acc\"                # Monitor for maximium val_acc or Minimium val_loss\n",
    "Mode = \"auto\"                      # max for val_acc and min for val_loss\n",
    "EARLY_STOPPING_PATIENCE = 15       # if the model doesnt improve after this many epochs then the training stops\n",
    "EPOCHs = 500                       # This is set high because we do early stopping\n",
    "\n",
    "##### FUNCTION 8 INPUTS #####\n",
    "\n",
    "##### FUNCTION 9 INPUTS #####\n",
    "\n",
    "##### IMT INPUTS #####\n",
    "COOL_DOWN = 25        # how long to sleep after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-84edf304f5ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mTrained_Models_Root_Path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./Trained_Models\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrained_Models_Root_Path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "###### SET THE INPUTS HERE  #########\n",
    "\n",
    "# Path to the directory that contains TRAINING_DIRECTORY, VALIDATION_DIRECTORY, and TEST_DIRECTORY\n",
    "Dell_Path = \"/home/hikmat/Desktop/DLWSpace/FAA_Guages_Data/\" # If training on the local machine\n",
    "# CHANGE C_Guage in main loop for name\n",
    "\n",
    "# MAIN FOR\n",
    "LEARNING_RATE_LST = [0.001]     #, 0.0009]#, 0.005, 0.009]  -> learning_rate -> LEARNING_RATE (4)\n",
    "Dropout_Rate_LST = [0.25]       #, 0.25]#, 0.5, 0.0]        -> dropout_rate -> Dropout_Rate (1)\n",
    "Is_To_Apply_Data_Augmentation_Lt = [False]#, False]         -> l_Is_To_Apply_Data_Augmentation -> Is_To_Apply_Data_Agumentation (5)\n",
    "\n",
    "# FUNCTION 1\n",
    "Is_To_Add_Dropout = False             #True\n",
    "Dense_Layers_Neurons = []             #[512] , tells how many neurons in the output dense layer\n",
    "Dropout_Rate = None\n",
    "\n",
    "# FUNCTION 2, Add_Conv_Regularizer()\n",
    "REGULARIZE_EARLY_LAYERS_WIHT_PERCENT = 0\n",
    "REGULARIZE_EARLY_LAYERS_WITH_RATIO = 0.001\n",
    "\n",
    "# FUNCTION 4, Compile_Model()\n",
    "LEARNING_RATE = None           # for setting the adam optimizer\n",
    "\n",
    "# FUNCTION 5, Create_Data_Loader()\n",
    "Is_To_Apply_Data_Augmentation = False\n",
    "\n",
    "\n",
    "##################################\n",
    "# TEST_CSV ... DO I NEED THIS?\n",
    "TEST_CSV_PATH = None\n",
    "TEST_CSV_Predicted_PATH = None\n",
    "TEST_CSV = None\n",
    "##################################\n",
    "\n",
    "\n",
    "# Start the main loop\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for learning_rate, dropout_rate, l_Is_To_Apply_Data_Augmentation in zip(LEARNING_RATE_LST, \n",
    "                                                Dropout_Rate_LST, Is_To_Apply_Data_Augmentation_Lt):\n",
    "        LEARNING_RATE = learning_rate\n",
    "        Dropout_Rate = dropout_rate\n",
    "        Is_To_Apply_Data_Augmentation = l_Is_To_Apply_Data_Augmentation\n",
    "        \n",
    "        # USED FOR NAMING THINGS?\n",
    "        C_Guage = \"HPE_Models\"\n",
    "\n",
    "        # Set the path to the trained models, if one doesn't exist -> create one\n",
    "        Trained_Models_Root_Path = \"./Trained_Models\"\n",
    "        if(os.path.exists(Trained_Models_Root_Path)):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(Trained_Models_Root_Path)\n",
    "        \n",
    "        # Choose where to store the Model info, FUNCTION 6\n",
    "        Store_Trained_ML_Model_Info_At = \"{0}/{1}\".format(Trained_Models_Root_Path, C_Guage)\n",
    "        \n",
    "        if(os.path.exists(Store_Trained_ML_Model_Info_At)):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(Store_Trained_ML_Model_Info_At)\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(\"*\" * 25)\n",
    "        print(\"HPE={0}\".format(C_Guage))\n",
    "\n",
    "\n",
    "        # Set the root path, in my case = to Dell_path\n",
    "        ROOT_DS_FOLDER_PATH = \"{0}\".format(Dell_Path)\n",
    "        \n",
    "        #########\n",
    "        # Might not need these\n",
    "        #TEST_CSV_PATH  = \"{1}/Training/CSV/{0}_ML_CSV_For_CNN_Test.csv\".format(C_Guage, Dell_Path)\n",
    "        #TEST_CSV_Predicted_PATH = \"{1}/Training/CSV/{0}_ML_Predicted_CSV_For_CNN_Test_{2}.csv\".format(\n",
    "        #    C_Guage, Dell_Path, str(datetime.now()))\n",
    "        #########\n",
    "\n",
    "        # Set the path to the training directory\n",
    "        TRAINING_DIRECTORY_PATH = \"{0}/TRAINING_DIRECTORY\".format(ROOT_DS_FOLDER_PATH)\n",
    "        # Define the number of classes based on how many subdirectories are in the Training Folder\n",
    "        OUTPUT_CLASSES = len(os.listdir(TRAINING_DIRECTORY_PATH))\n",
    "        # Map the classes 0-8\n",
    "        CLASS_MAPPINGS = []\n",
    "        for label in range(0, OUTPUT_CLASSES):\n",
    "            CLASS_MAPPINGS.append(str(label))\n",
    "            \n",
    "        # Set the path to the validation and test directories    \n",
    "        VALIDATION_DIRECTORY_PATH = \"{0}/VALIDATION_DIRECTORY\".format(ROOT_DS_FOLDER_PATH)\n",
    "        TESTING_DIRECTORY_PATH = \"{0}/TESTING_DIRECTORY\".format(ROOT_DS_FOLDER_PATH)\n",
    "        \n",
    "        # print the paths\n",
    "        print(\"Current TRAINING_DIRECTORY_PATH={0}\".format(TRAINING_DIRECTORY_PATH))\n",
    "        print(\"Current VALIDATION_DIRECTORY_PATH={0}\".format(VALIDATION_DIRECTORY_PATH))\n",
    "        print(\"Current TESTING_DIRECTORY_PATH={0}\".format(TESTING_DIRECTORY_PATH))\n",
    "        \n",
    "        ########\n",
    "        # This could be for evaluation\n",
    "        #print(\"PREDICTED_CSV_WILL_BE_SAVED_AT_PATH=\" + str(TEST_CSV_Predicted_PATH))\n",
    "        #print(\"TEST_CSV_PATH={0}\".format(TEST_CSV_PATH))\n",
    "        #TEST_CSV = pd.read_csv(TEST_CSV_PATH)\n",
    "        #print(\"*\" * 25)\n",
    "        ########\n",
    "\n",
    "\n",
    "        \n",
    "###########################################################################################\n",
    "# THIS SECTION OF THE CODE IS WHAT ACTUALLY RUNS SOMETHING\n",
    "# MUST DEFINE BATCH_SIZE, INPUT_SIZE, SELECTED MODEL (from list of Available_Models)\n",
    "# MAIN CODE IS IN THE Initiate_Model_Training FUNCTION\n",
    "        \n",
    "        BATCH_SIZE = 96 * AVAILABLE_GPUs\n",
    "        INPUT_SIZE = (224,224, 3)\n",
    "        SELECTED_MODEL = Available_Models.ResNet50\n",
    "        Initiate_Model_Training()\n",
    "        print(\"Saving Prediction of {0}\".format(SELECTED_MODEL))\n",
    "\n",
    "#             BATCH_SIZE = 1024 * AVAILABLE_GPUs\n",
    "#             INPUT_SIZE = (64,64, 3)\n",
    "#             SELECTED_MODEL = Available_Models.Lenet_Style\n",
    "#             Initiate_Model_Training()\n",
    "\n",
    "#             BATCH_SIZE = 96 * AVAILABLE_GPUs\n",
    "#             INPUT_SIZE = (224,224, 3)\n",
    "#             SELECTED_MODEL = Available_Models.VGG19\n",
    "#             Initiate_Model_Training()\n",
    "#             print(\"Saving Prediction of {0}\".format(SELECTED_MODEL))\n",
    "#             TEST_CSV.to_csv(TEST_CSV_Predicted_PATH)\n",
    "\n",
    "\n",
    "#        BATCH_SIZE = 2048 * AVAILABLE_GPUs\n",
    "#        # INPUT SIZE GOES TO FUNCTION 5\n",
    "#        INPUT_SIZE = (64,64, 3)\n",
    "#        SELECTED_MODEL = Available_Models.ResNet18\n",
    "#        Initiate_Model_Training()\n",
    "#        print(\"Saving Prediction of {0}\".format(SELECTED_MODEL))\n",
    "#        TEST_CSV.to_csv(TEST_CSV_Predicted_PATH)\n",
    "\n",
    "\n",
    "        #BATCH_SIZE = 512 * AVAILABLE_GPUs\n",
    "        #INPUT_SIZE = (128,128, 3)\n",
    "        #SELECTED_MODEL = Available_Models.ResNet34\n",
    "        #Initiate_Model_Training()\n",
    "        #print(\"Saving Prediction of {0}\".format(SELECTED_MODEL))\n",
    "        #TEST_CSV.to_csv(TEST_CSV_Predicted_PATH)\n",
    "\n",
    "\n",
    "\n",
    "#             BATCH_SIZE = 96 * AVAILABLE_GPUs\n",
    "#             INPUT_SIZE = (224,224, 3)\n",
    "#             SELECTED_MODEL = Available_Models.ResNet50\n",
    "#             Initiate_Model_Training()\n",
    "#             print(\"Saving Prediction of {0}\".format(SELECTED_MODEL))\n",
    "#             TEST_CSV.to_csv(TEST_CSV_Predicted_PATH)\n",
    "\n",
    "\n",
    "#             BATCH_SIZE = 32 * AVAILABLE_GPUs\n",
    "#             INPUT_SIZE = (299,299, 3)\n",
    "#             SELECTED_MODEL = Available_Models.Xception\n",
    "#             Initiate_Model_Training()\n",
    "#             print(\"Saving Prediction of {0}\".format(SELECTED_MODEL))\n",
    "\n",
    "\n",
    "#             BATCH_SIZE = 32 * AVAILABLE_GPUs\n",
    "#             INPUT_SIZE = (299,299, 3)\n",
    "#             SELECTED_MODEL = Available_Models.InceptionV3\n",
    "#             Initiate_Model_Training()\n",
    "\n",
    "#             BATCH_SIZE = 32 * AVAILABLE_GPUs\n",
    "#             INPUT_SIZE = (299,299, 3)\n",
    "#             SELECTED_MODEL = Available_Models.InceptionResNetV2\n",
    "#             Initiate_Model_Training()\n",
    "\n",
    "#             BATCH_SIZE = 32 * AVAILABLE_GPUs\n",
    "#             INPUT_SIZE = (224,224, 3)\n",
    "#             SELECTED_MODEL = Available_Models.DenseNet121\n",
    "#             Initiate_Model_Training()\n",
    "\n",
    "#             BATCH_SIZE = 96 * AVAILABLE_GPUs\n",
    "#             INPUT_SIZE = (224,224, 3)\n",
    "#             SELECTED_MODEL = Available_Models.VGG16\n",
    "#             Initiate_Model_Training()\n",
    "\n",
    "#             print(\"SAVING_PREDICTED_CSV_TO_PATH=\" + str(TEST_CSV_Predicted_PATH))\n",
    "        #TEST_CSV.to_csv(TEST_CSV_Predicted_PATH)\n",
    "        #TEST_CSV = None\n",
    "        #TEST_CSV_PATH = None\n",
    "        #TEST_CSV_Predicted_PATH = None\n",
    "        #print(\"*\" * 25)\n",
    "        print(\"{0} Training Has Completed!\".format(C_Guage))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
